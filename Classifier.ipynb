{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\python312\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python312\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3860\\298026809.py:12: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  embeddings_path = '.\\dataset\\W2V_150.txt'\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]  # Từ là giá trị đầu tiên\n",
    "            vector = np.array(list(map(float, values[1:])))  # Các giá trị tiếp theo là vector embedding\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Đọc embedding từ file W2V_150.txt\n",
    "embeddings_path = '.\\dataset\\W2V_150.txt'\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "def get_word_vector(word):\n",
    "    if word in embeddings:\n",
    "        return embeddings[word]\n",
    "    else:\n",
    "        return np.zeros(150)  # Assuming 300-dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2578754 , -0.9726154 , -1.724687  ,  0.1039191 ,  0.7886895 ,\n",
       "       -0.9466447 ,  0.4693937 ,  1.152593  ,  0.1155706 ,  0.1692931 ,\n",
       "        0.7421699 , -0.787329  , -0.6093806 ,  1.025403  ,  0.6054297 ,\n",
       "       -1.236188  , -0.9221155 , -1.210821  , -0.2781016 ,  1.110141  ,\n",
       "        1.728644  , -0.8907079 , -0.7430073 , -1.386758  , -0.6328576 ,\n",
       "        0.4824337 , -0.4921961 , -0.5655223 , -1.927649  , -0.4070618 ,\n",
       "        0.181633  ,  0.7142181 , -0.8746694 ,  0.8621774 ,  0.3763472 ,\n",
       "       -0.2323172 , -1.696975  , -0.6795936 , -0.7101342 , -0.3980412 ,\n",
       "        0.8449571 , -0.9750128 ,  1.558469  , -1.674035  , -0.680132  ,\n",
       "       -0.97935   ,  0.07189509, -0.2478063 , -0.4585635 , -0.6954154 ,\n",
       "        2.041032  ,  2.368579  ,  0.9618701 ,  1.230371  ,  2.721321  ,\n",
       "        0.5995511 ,  0.8380752 ,  0.8573769 ,  1.95823   ,  1.082221  ,\n",
       "        0.6791626 , -0.9185423 ,  0.3077531 ,  0.2545303 , -1.09807   ,\n",
       "        0.07830691, -0.7900249 , -0.3596387 , -0.5374528 ,  0.7045696 ,\n",
       "       -1.405178  ,  0.748323  , -0.9748238 ,  0.505672  , -1.064186  ,\n",
       "       -0.09893318, -0.3444446 , -1.785952  ,  2.029789  , -0.6841028 ,\n",
       "       -3.018026  , -0.0218592 , -0.03212921, -0.8394837 ,  0.1017763 ,\n",
       "        1.469285  ,  0.517174  , -0.8083785 , -0.7863334 , -0.02706276,\n",
       "        1.647663  , -0.4082772 , -1.531167  , -1.613358  , -1.267038  ,\n",
       "        0.4993207 , -0.5925277 , -0.3438716 , -1.337588  , -0.03321799,\n",
       "       -0.5446152 ,  0.6069489 , -1.289287  ,  0.4370298 ,  2.350519  ,\n",
       "       -0.08258818, -0.5376267 , -1.415858  ,  1.383907  , -1.662229  ,\n",
       "        0.2631687 ,  1.319684  , -0.8904743 , -1.655962  , -2.524005  ,\n",
       "       -0.7317258 , -3.200265  ,  0.2673278 , -1.427984  ,  1.147003  ,\n",
       "        0.6494801 , -1.415339  ,  0.2135608 ,  1.755648  ,  0.4420949 ,\n",
       "       -0.4785798 , -1.011857  ,  0.4852979 , -1.321548  ,  2.474401  ,\n",
       "        0.657818  ,  0.5786278 ,  1.582759  , -1.149667  , -2.565566  ,\n",
       "       -2.116292  ,  0.8363985 ,  0.4458983 ,  0.4037514 , -1.38201   ,\n",
       "       -0.2172358 , -1.529086  , -0.3343721 ,  0.3834576 ,  0.697854  ,\n",
       "        0.4732894 , -0.6912606 ,  1.344066  ,  0.05758414,  0.7924209 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_vector('chó')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping line, not enough values: ['bằng_chứng']\n",
      "Skipping line, not enough values: ['dài_ngoằng']\n",
      "Skipping line, not enough values: ['dầu_nhớt']\n",
      "Skipping line, not enough values: ['diễn_dịch']\n",
      "Processed pairs: 13556, Labels: 13556\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "pairs = []\n",
    "labels = []\n",
    "\n",
    "with open('./antonym-synonym/Antonym_vietnamese.txt', 'r', encoding='utf-8') as f:\n",
    "    next(f)  # Skip the header line if present\n",
    "    for line in f:\n",
    "        if line.strip() == \"\":  # Skip empty lines\n",
    "            continue\n",
    "        values = line.strip().split(' ')\n",
    "        \n",
    "        # Check if we have at least two values to unpack\n",
    "        if len(values) < 2:\n",
    "            print(f\"Skipping line, not enough values: {values}\")  # Debug line\n",
    "            continue\n",
    "        \n",
    "        word1 = values[0]\n",
    "        word2 = values[1]\n",
    "        \n",
    "        # Get the word vectors\n",
    "        vector1 = get_word_vector(word1)\n",
    "        vector2 = get_word_vector(word2)\n",
    "        \n",
    "        # Assuming the vectors are of the same size\n",
    "        vector = np.concatenate([vector1, vector2])\n",
    "        pairs.append(vector)\n",
    "        labels.append(0)  # Assuming 0 for antonyms\n",
    "\n",
    "\n",
    "with open('./antonym-synonym/Synonym_vietnamese.txt', 'r', encoding='utf-8') as f:\n",
    "    next(f)  # Skip the header line if present\n",
    "    for line in f:\n",
    "        if line.strip() == \"\":  # Skip empty lines\n",
    "            continue\n",
    "        values = line.strip().split(' ')\n",
    "        \n",
    "        # Check if we have at least two values to unpack\n",
    "        if len(values) < 2:\n",
    "            print(f\"Skipping line, not enough values: {values}\")  # Debug line\n",
    "            continue\n",
    "        \n",
    "        word1 = values[0]\n",
    "        word2 = values[1]\n",
    "        \n",
    "        # Get the word vectors\n",
    "        vector1 = get_word_vector(word1)\n",
    "        vector2 = get_word_vector(word2)\n",
    "        \n",
    "        # Assuming the vectors are of the same size\n",
    "        vector = np.concatenate([vector1, vector2])\n",
    "        pairs.append(vector)\n",
    "        labels.append(1)  # Assuming 1 for synonym, 0 for antonym\n",
    "\n",
    "# After processing, you can print out the pairs and labels to check\n",
    "print(f\"Processed pairs: {len(pairs)}, Labels: {len(labels)}\")\n",
    "\n",
    "# Load train and test data\n",
    "X_train, y_train = np.array(pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            \n",
    "            values = line.split()  # Split by spaces\n",
    "            if len(values) < 3:  # Ensure there are at least three values\n",
    "                print(f\"Skipping line, not enough values: {values}\")  # Debug line\n",
    "                continue\n",
    "            \n",
    "            word1 = values[0]\n",
    "            word2 = values[1]\n",
    "            label = values[2]  # Keep as string for comparison\n",
    "            \n",
    "            # Get the word vectors\n",
    "            vector1 = get_word_vector(word1)\n",
    "            vector2 = get_word_vector(word2)\n",
    "            \n",
    "            # Combine the vectors\n",
    "            vector = np.concatenate([vector1, vector2])\n",
    "            pairs.append(vector)\n",
    "            labels.append(0 if label == 'ANT' else 1)  # 1 for synonym, 0 for antonym\n",
    "\n",
    "    return np.array(pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "clf_logistic = LogisticRegression(max_iter=1000)\n",
    "clf_logistic.fit(X_train, y_train)\n",
    "\n",
    "# MLP Classifier\n",
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, activation='relu')\n",
    "clf_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "def evaluate(y_true, y_pred, model_name):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"{model_name} - Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Precision: 0.569364161849711, Recall: 0.985, F1-Score: 0.7216117216117216\n",
      "MLP - Precision: 0.9803921568627451, Recall: 1.0, F1-Score: 0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "X_test, y_test = load_data('./dataset/ViCon-400/400_noun_pairs.txt')\n",
    "len(X_test), len(y_test)\n",
    "y_pred_logistic = clf_logistic.predict(X_test)\n",
    "y_pred_mlp = clf_mlp.predict(X_test)\n",
    "# Logistic Regression Evaluation\n",
    "evaluate(y_test, y_pred_logistic, 'Logistic Regression')\n",
    "\n",
    "# MLP Evaluation\n",
    "evaluate(y_test, y_pred_mlp, 'MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_verb_test, y_verb_test = load_data('./dataset/ViCon-400/400_verb_pairs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - Precision: 0.9852941176470589, Recall: 1.0, F1-Score: 0.9925925925925926\n",
      "Logistic Regression - Precision: 0.599388379204893, Recall: 0.9751243781094527, F1-Score: 0.7424242424242424\n"
     ]
    }
   ],
   "source": [
    "ypred_mlpverb = clf_mlp.predict(X_verb_test)\n",
    "ypred_logisticverb = clf_logistic.predict(X_verb_test)\n",
    "evaluate(y_verb_test, ypred_mlpverb, 'MLP')\n",
    "evaluate(y_verb_test, ypred_logisticverb, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adj_test, y_adj_test = load_data('./dataset/ViCon-400/600_adj_pairs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Precision: 0.6953125, Recall: 0.8870431893687708, F1-Score: 0.7795620437956204\n",
      "MLP - Precision: 1.0, Recall: 1.0, F1-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "ypred_mlpadj = clf_mlp.predict(X_adj_test)\n",
    "ypred_logisticadj = clf_logistic.predict(X_adj_test)\n",
    "evaluate(y_adj_test, ypred_logisticadj, 'Logistic Regression')\n",
    "evaluate(y_adj_test, ypred_mlpadj, 'MLP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
